{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Driven_RiskScoreModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvk-xfT9-_cu"
      },
      "source": [
        "#@title PySpark Setup(run me!)\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI7B6tS-_B-Q"
      },
      "source": [
        "#@title Dynamic Input(select your choices and run the cell)\n",
        "Date_to_calculate_age = '2020-01-01' #@param {type:\"date\"}\n",
        "Choose_a_model_version = 'V24' #@param [\"V22\",\"V24\", \"RX\", \"ESRD-P1\",\"ESRD-P2\"]\n",
        "Choose_a_model_year = '2020' #@param [\"2020\",\"2021\"]\n",
        "Sex_Age_edits_required = 'Yes' #@param [\"Yes\",\"No\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va0FzvuF_IEE"
      },
      "source": [
        "#@title Age Calculation\n",
        "import pandas \n",
        "import os\n",
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql.functions import datediff, to_date, lit\n",
        "from pyspark.sql.functions import struct\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "\n",
        "#Loading Persons file\n",
        "df1 = pandas.read_csv (\"/content/Person-file2.csv\")\n",
        "#df1=df.drop(['Unnamed: 8', 'Unnamed: 9','Unnamed: 10'], axis=1)\n",
        "df1 = spark.createDataFrame(df1)\n",
        "\n",
        "df2=df1.withColumn(\"Age\",(datediff(to_date(lit(Date_to_calculate_age)),(\"DOB\")))/366)\n",
        "df3 = df2.withColumn(\"Age\", func.round(df2[\"Age\"], 2).cast('integer'))\n",
        "\n",
        "#Condition for AGE and OREC\n",
        "def func(Age, OREC):\n",
        "  if Age == 64 and OREC == 0:\n",
        "    return 65\n",
        "  elif Age < 0 :\n",
        "    return 0\n",
        "  return Age\n",
        "\n",
        "\n",
        "func_udf = udf(func, IntegerType())\n",
        "df4 = df3.withColumn('new_column',func_udf(df3['Age'], df3['OREC']))\n",
        "\n",
        "drop_list = [ 'Age',]\n",
        "sdf5=df4.select([column for column in df4.columns if column not in drop_list])\n",
        "\n",
        "df_final = sdf5.withColumnRenamed(\"new_column\", \"Age\")\n",
        "\n",
        "#df_final.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5YHR0iP_IGT"
      },
      "source": [
        "#@title Demography Variable\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import datediff, to_date, lit\n",
        "from pyspark.sql.functions import col, expr, when\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "data = df_final\n",
        "\n",
        "demo = (spark.read.format(\"csv\").options(header=\"true\").load(\"/content/Demography Variable Calculations.csv\"))\n",
        "\n",
        "demo = demo.filter((demo['Version'] == Choose_a_model_version) & (demo['Year'] == Choose_a_model_year))\n",
        "data = data.withColumn(\"Year\", lit(str(Choose_a_model_year)))\n",
        "data = data.withColumn(\"Version\", lit(str(Choose_a_model_version)))\n",
        "\n",
        "for i in demo.collect():\n",
        "  data = data.withColumn(i[2], expr(i[3]))\n",
        "\n",
        "#data.show()\n",
        "\n",
        "#data=data.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSJvOhXo_IHx"
      },
      "source": [
        "data = data.withColumnRenamed(\"Version\",\"V\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8G__XWc_ILU"
      },
      "source": [
        "#@title CC_Mapping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from pyspark.sql.functions import length,col,trim\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_diagnosis = pd.read_csv('/content/Person_DiagonsisCodes2.csv')\n",
        "df_diagnosis = spark.createDataFrame(df_diagnosis)\n",
        "df_diagnosis = df_diagnosis.withColumn('DIAGNOSIS CODE',trim(col(\"DIAGNOSIS CODE\")))\n",
        "\n",
        "df_CC_mapping = pd.read_csv('/content/CC Mapping.csv')\n",
        "label_schema = StructType([\n",
        "    StructField(\"YEAR\", StringType()),\n",
        "    StructField(\"VERSION\", StringType()),\n",
        "    StructField(\"DIAGNOSIS CODE\", StringType()),\n",
        "    StructField(\"CC\", IntegerType())\n",
        "])\n",
        "df_CC_mapping = spark.createDataFrame(df_CC_mapping,schema= label_schema)\n",
        "\n",
        "\n",
        "start=time.time()\n",
        "# for year in model_years:\n",
        "#   for version in versions:\n",
        "cond1= col('YEAR')== str(Choose_a_model_year)\n",
        "cond2= col('VERSION')== str(Choose_a_model_version)\n",
        "df_temp= df_CC_mapping.where(cond1 & cond2)\n",
        "\n",
        "# df_temp.show(5)\n",
        "df_temp= df_temp.toDF('YEAR','VERSION','D_DIAG CODE','CC')\n",
        "\n",
        "h= df_temp.join(df_diagnosis,(df_diagnosis[\"DIAGNOSIS CODE\"] == df_temp[\"D_DIAG CODE\"]),how='right')  # Mtachin diagnosis codes from diagnosis input file and CC_mapping file\n",
        "\n",
        "unique_HCC= sorted([i.CC for i in df_temp.select('CC').distinct().collect()]) #Finding all unique values of HCC and storing it in a list to generate HCC columns later\n",
        "\n",
        "CC_list = h.select(\"CC\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "p_id = h.select(\"PERSON ID\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "diag_code= h.select(\"DIAGNOSIS CODE\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "\n",
        "df_main= pd.DataFrame()\n",
        "df_main['PERSON ID']= p_id\n",
        "df_main['DIAGNOSIS CODE']= diag_code \n",
        "\n",
        "df_main['HCC']= CC_list  # CC values obtained after mapping from Part 2.\n",
        "df_main['HCC'] = list(df_main['HCC'].astype(np.float).astype(\"Int32\"))\n",
        "\n",
        "df_main['MODEL YEAR']= str(Choose_a_model_year)\n",
        "df_main['Version']=str(Choose_a_model_version)\n",
        "\n",
        "# df_main\n",
        "if 'RX' in Choose_a_model_version:\n",
        "  columns_df=[]\n",
        "  for value in (unique_HCC):\n",
        "    columns_df.append('RXHCC'+str(value)) # Example: RXHCC1,RXHCC2,RXHCC3 etc\n",
        "  #print(columns_df)\n",
        "  for colum in range(len(columns_df)):\n",
        "    df_main[columns_df[colum]]=0  #adding zero's to every column \n",
        "\n",
        "  for v in range(len(df_main)):                         \n",
        "    df_main.at[v,'RXHCC'+ str(df_main.iloc[v][2])]=1   # Adding 1's to columns. Example if df_main['HCC'][10]= 22, then adding a 1 in HCC10 column in the 9th row. \n",
        "\n",
        "  df_main = df_main.drop(df_main.columns[-1],axis=1)\n",
        "  #df_main.to_excel(str(Choose_a_model_version)+'_'+str(Choose_a_model_year)+'.xlsx')\n",
        "\n",
        "\n",
        "else:\n",
        "  # Part 5: Creating new columns with column names as HCC values obtained from Part 4\n",
        "  columns_df=[]\n",
        "  for value in (unique_HCC):\n",
        "    columns_df.append('HCC'+str(value)) # Example: HCC1,HCC2,HCC3 etc\n",
        "  #print(columns_df)\n",
        "  for colum in range(len(columns_df)):\n",
        "    df_main[columns_df[colum]]=0  #adding zero's to every column \n",
        "\n",
        "  for v in range(len(df_main)):                         \n",
        "    df_main.at[v,'HCC'+ str(df_main.iloc[v][2])]=1   # Adding 1's to columns. Example if df_main['HCC'][10]= 22, then adding a 1 in HCC10 column in the 9th row. \n",
        "\n",
        "  df_main = df_main.drop(df_main.columns[-1],axis=1)\n",
        "  #df_main.to_excel(str(Choose_a_model_version)+'_'+str(Choose_a_model_year)+'.xlsx')\n",
        "\n",
        "\n",
        "df_main= df_main.fillna(0)\n",
        "spark_df_main= spark.createDataFrame(df_main)\n",
        "#spark_df_main.show(5)\n",
        "print(time.time()-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdMIqZrq_IPd"
      },
      "source": [
        "@title CC-Override\n",
        "data = data.withColumnRenamed(\"Person ID\", \"PERSON ID\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52hWxwYa_IR3"
      },
      "source": [
        "df = data.join(spark_df_main, 'PERSON ID', 'inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSeO6yUF_IUC"
      },
      "source": [
        "columns_to_drop = ['Year','V']\n",
        "df= df.drop(*columns_to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iABK-n_8_IWJ"
      },
      "source": [
        "df=df.withColumnRenamed(\"DIAGNOSIS CODE\",\"DIAG\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvlH27L4_IYa"
      },
      "source": [
        "df=df.withColumnRenamed(\"Gender\",\"SEX\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nApsMczR_Iaq"
      },
      "source": [
        "df=df.withColumnRenamed(\"Age\",\"AGEF\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IZe87qj_Icx"
      },
      "source": [
        "data = df\n",
        "    \n",
        "overide_df = (spark.read.format(\"csv\").options(header=\"true\").load(\"/content/CC Override Rules.csv\"))\n",
        "overide_df = overide_df.withColumnRenamed(\"MODEL YEAR\",\"MODEL_YEAR\")\n",
        "overide_df = overide_df.filter((overide_df.VERSION.isin(Choose_a_model_version))&(overide_df.MODEL_YEAR.isin(Choose_a_model_year)))\n",
        "for i in overide_df.collect():\n",
        "  data = data.withColumn('HCC', expr(i[3]))\n",
        "#data.toPandas().to_excel('Merged_CCOveride.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGWVCTaX_IfB"
      },
      "source": [
        "df_CCmerged= data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjXJTdAc_Igy"
      },
      "source": [
        "@title HCC HIERARCHY\n",
        "data = pd.read_excel('/content/HCC Hierarchy.xlsx')   #Loading Files\n",
        "data1 = spark.createDataFrame(data) \n",
        "data_2020_test = data.loc[(data['MODEL YR'] == int(Choose_a_model_year)) & (data['MODEL VRSN'] == Choose_a_model_version)]\n",
        "merged_cc_2020_test = df_CCmerged\n",
        "for i in range(len(data_2020_test)):\n",
        "  # merged_cc_2020_spark1 = 0\n",
        "  # if list(data_2020_test['HIGHER HCC'])[i] in merged_cc_2020_test.columns:\n",
        "  merged_cc_2020_spark1 = merged_cc_2020_test.withColumn(list(data_2020_test['LOWER HCC'])[i], F.when((F.col(list(data_2020_test['HIGHER HCC'])[i])==1) & (F.col(list(data_2020_test['LOWER HCC'])[i])== 1) & (F.col('VERSION')== list(data_2020_test['MODEL VRSN'])[i]) ,0).otherwise(F.col(list(data_2020_test['LOWER HCC'])[i])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5xgC2H3_IkK"
      },
      "source": [
        "result= merged_cc_2020_spark1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7vbwxFD__Oc"
      },
      "source": [
        "@title Interaction Variable Calculation\n",
        " #loading libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import pyspark.sql.functions as func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rolFPAmj__Qk"
      },
      "source": [
        "from pyspark.sql.functions import datediff, to_date, lit\n",
        "from pyspark.sql.functions import col, expr, when\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "#result= pd.read_csv('/content/Merged_file_for_HCC_updated.csv')\n",
        "\n",
        "data_interaction = result\n",
        "\n",
        "interaction_demo = pd.read_excel(\"/content/Interaction Variable Calculations.xlsx\")\n",
        "interaction_demo= spark.createDataFrame(interaction_demo)\n",
        "\n",
        "interaction_demo = interaction_demo.filter((interaction_demo['MODEL VRSN'] == Choose_a_model_version) & (interaction_demo['MODEL YR'] == Choose_a_model_year))\n",
        "data_interaction = data_interaction.where(F.col('MODEL YEAR') == Choose_a_model_year)\n",
        "data_interaction = data_interaction.withColumn(\"Version\", lit(str(Choose_a_model_version)))\n",
        "\n",
        "for i in interaction_demo.collect():\n",
        "  if i[2] in result.columns:\n",
        "      data_interaction = data_interaction.withColumn(i[2], expr(i[3]))\n",
        "\n",
        "# data_interaction.show(5)\n",
        "# data_interaction.toPandas().to_excel('Interaction_Variable_Calculation.xlsx') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5uaIBWS__TH"
      },
      "source": [
        "@title Score Calculation\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElGWviou__VA"
      },
      "source": [
        "columns_to_drop = ['DOB', 'SEX', 'OREC', 'LTIMCAID', 'NEMCAID', 'ESRD', 'MCAID', 'AGEF', 'HCC']\n",
        "data_interaction = data_interaction.drop(*columns_to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzRnsOHq__Y8"
      },
      "source": [
        "dfColumnNames = data_interaction.schema.names\n",
        "print(dfColumnNames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BZqICCt__cL"
      },
      "source": [
        "data_interaction = data_interaction.select('DIAG', 'MODEL YEAR', 'Version', 'PERSON ID','DISABL','F0_34', 'F35_44', 'F45_54', 'F55_59', 'F60_64', 'F65_69', 'F70_74', 'F75_79', 'F80_84', 'F85_89', 'F90_94', 'F95_GT', 'LTIMCAID_O', 'M0_34', 'M35_44', 'M45_54', 'M55_59', 'M60_64', 'M65_69', 'M70_74', 'M75_79', 'M80_84', 'M85_89', 'M90_94', 'M95_GT', 'MCAID_NORIGDIS  ', 'MCAID_ORIGDIS   ', 'NE_ORIGDS', 'NEF0_34', 'NEF35_44', 'NEF45_54', 'NEF55_59', 'NEF60_64', 'NEF65', 'NEF66', 'NEF67', 'NEF68', 'NEF69', 'NEF70_74', 'NEF75_79', 'NEF80_84', 'NEF85_89', 'NEF90_94', 'NEF95_GT', 'NEM0_34', 'NEM35_44', 'NEM45_54', 'NEM55_59', 'NEM60_64', 'NEM65', 'NEM66', 'NEM67', 'NEM68', 'NEM69', 'NEM70_74', 'NEM75_79', 'NEM80_84', 'NEM85_89', 'NEM90_94', 'NEM95_GT', 'NMCAID_NORIGDIS ', 'NMCAID_ORIGDIS  ', 'ORIGDS', 'HCC1', 'HCC2', 'HCC6', 'HCC8', 'HCC9', 'HCC10', 'HCC11', 'HCC12', 'HCC17', 'HCC18', 'HCC19', 'HCC21', 'HCC22', 'HCC23', 'HCC27', 'HCC28', 'HCC29', 'HCC33', 'HCC34', 'HCC35', 'HCC39', 'HCC40', 'HCC46', 'HCC47', 'HCC48', 'HCC51', 'HCC52', 'HCC54', 'HCC55', 'HCC56', 'HCC57', 'HCC58', 'HCC59', 'HCC60', 'HCC70', 'HCC71', 'HCC72', 'HCC73', 'HCC74', 'HCC75', 'HCC76', 'HCC77', 'HCC78', 'HCC79', 'HCC80', 'HCC82', 'HCC83', 'HCC84', 'HCC85', 'HCC86', 'HCC87', 'HCC88', 'HCC96', 'HCC99', 'HCC100', 'HCC103', 'HCC104', 'HCC106', 'HCC107', 'HCC108', 'HCC110', 'HCC111', 'HCC112', 'HCC114', 'HCC115', 'HCC122', 'HCC124', 'HCC134', 'HCC135', 'HCC136', 'HCC137', 'HCC138', 'HCC157', 'HCC158', 'HCC159', 'HCC161', 'HCC162', 'HCC166', 'HCC167', 'HCC169', 'HCC170', 'HCC173', 'HCC176', 'HCC186', 'HCC188', 'HCC189')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUbJFWwRAcHi"
      },
      "source": [
        "dfColumnNames = data_interaction.schema.names\n",
        "print(dfColumnNames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjyftKmbAl8a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coBsZ5QDAdD_"
      },
      "source": [
        "# Fetch score sheet\n",
        "scoreSheetPath = '/content/drive/MyDrive/Index/Score Variables.xlsx'   #Path to access the score variables from drive\n",
        "scoreSheetDF = pd.read_excel(scoreSheetPath)\n",
        "del scoreSheetDF['Unnamed: 5'],scoreSheetDF['Unnamed: 6'],scoreSheetDF['Unnamed: 7'],scoreSheetDF['Unnamed: 8'],scoreSheetDF['Unnamed: 9'],scoreSheetDF['Unnamed: 10'],scoreSheetDF['Unnamed: 11'],scoreSheetDF['Unnamed: 12']\n",
        "# scoreSheetDF = spark.createDataFrame(scoreSheetDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQmKR48FAdGl"
      },
      "source": [
        "scoreSheetDF = scoreSheetDF.loc[(scoreSheetDF['Model Year'] == int(Choose_a_model_year)) & (scoreSheetDF['Model Version'] == Choose_a_model_version)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elduXUD-AdKh"
      },
      "source": [
        "for i in range(len(list(scoreSheetDF['Variable']))):\n",
        "\n",
        "  if list(scoreSheetDF['Variable'])[i] in data_interaction.columns:  \n",
        "    data_interaction = data_interaction.withColumn(list(scoreSheetDF['Variable'])[i], F.when((F.col(list(scoreSheetDF['Variable'])[i])==1)  ,(list(scoreSheetDF['Coefficient Value'])[i])).otherwise(0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zYZbH9vAdN3"
      },
      "source": [
        "data_interaction.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}